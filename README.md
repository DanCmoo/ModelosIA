# Modelo de PredicciÃ³n de ConcentraciÃ³n de Cloro con LSTM

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un modelo de predicciÃ³n de concentraciÃ³n de cloro usando **LSTM (Long Short-Term Memory)** basado en el **Teorema de Takens** para reconstrucciÃ³n del espacio de fases. El proyecto sigue estrictamente las reglas y especificaciones de la guÃ­a tÃ©cnica.

## ğŸ¯ Objetivos

- Predecir la concentraciÃ³n de cloro en un sistema de tratamiento de agua
- Demostrar que la IA puede aprender dinÃ¡micas sin conocer las ecuaciones subyacentes
- Cumplir con todas las reglas de implementaciÃ³n (G1-G4, T1-T3, L1-L5, V1-V2)

## ğŸ”¬ Fundamento TeÃ³rico

### Teorema de Takens
El proyecto se basa en el teorema de reconstrucciÃ³n del espacio de fases, que establece que una serie temporal escalar `s(t)` puede ser embebida en un espacio de dimensiÃ³n superior que preserva las propiedades dinÃ¡micas del sistema original:

**X**áµ¢ = [s(i), s(i+Ï„), s(i+2Ï„), ..., s(i+(d-1)Ï„)]

Donde:
- **Ï„** = retardo temporal (calculado por autocorrelaciÃ³n)
- **d** = dimensiÃ³n de embebimiento (calculado por False Nearest Neighbors)

## ğŸ“ Estructura del Proyecto

```
ModelosIA/
â”œâ”€â”€ cloro/                           # Modelo de ConcentraciÃ³n de Cloro
â”‚   â”œâ”€â”€ generar_datos_cloro.py      # Genera serie temporal y guarda CSV
â”‚   â”œâ”€â”€ modelo_cloro_lstm.py         # Modelo LSTM principal
â”‚   â”œâ”€â”€ ejecutar_pipeline.py         # Script para ejecutar todo el pipeline
â”‚   â”œâ”€â”€ comparar_modelos.py          # ComparaciÃ³n modelo fÃ­sico vs LSTM
â”‚   â”œâ”€â”€ README.md                    # DocumentaciÃ³n del modelo de cloro
â”‚   â”œâ”€â”€ datos_cloro.csv              # Serie temporal generada
â”‚   â”œâ”€â”€ modelo_lstm_cloro.h5         # Modelo entrenado
â”‚   â”œâ”€â”€ datos_cloro_visualizacion.png
â”‚   â”œâ”€â”€ lstm_cloro_resultados.png
â”‚   â””â”€â”€ comparacion_modelo_fisico_vs_lstm.png
â”œâ”€â”€ documentos/
â”‚   â””â”€â”€ guia_tecnica_reglas.md       # GuÃ­a de implementaciÃ³n
â”œâ”€â”€ pyproject.toml                   # ConfiguraciÃ³n UV/Python
â”œâ”€â”€ requirements.txt                 # Dependencias (pip)
â”œâ”€â”€ INSTALACION_UV.md                # GuÃ­a de instalaciÃ³n con UV
â””â”€â”€ README.md                        # Este archivo
```

## ğŸš€ InstalaciÃ³n

### Requisitos
- Python 3.8+
- TensorFlow 2.x
- NumPy, Pandas, Scikit-learn, Matplotlib, SciPy

### OpciÃ³n 1: Instalar con UV (Recomendado - âš¡ MÃ¡s rÃ¡pido)

```bash
# Instalar UV si no lo tienes
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Instalar dependencias
uv venv
uv pip install -e .
```

Ver [INSTALACION_UV.md](INSTALACION_UV.md) para mÃ¡s detalles.

### OpciÃ³n 2: Instalar con pip

```bash
pip install -r requirements.txt
```

## ğŸ’» Uso

### OpciÃ³n 1: Con UV (Recomendado)

```bash
# Navegar a la carpeta del modelo
cd cloro

# Pipeline completo
uv run --no-project ejecutar_pipeline.py

# O paso a paso
uv run --no-project generar_datos_cloro.py
uv run --no-project modelo_cloro_lstm.py
```

### OpciÃ³n 2: Pipeline Completo (pip/entorno tradicional)

```bash
cd cloro
python ejecutar_pipeline.py
```

Este script ejecuta automÃ¡ticamente:
1. GeneraciÃ³n de datos
2. Entrenamiento del modelo LSTM
3. EvaluaciÃ³n y visualizaciÃ³n

### OpciÃ³n 3: EjecuciÃ³n Paso a Paso

```bash
cd cloro

# Paso 1: Generar datos
python generar_datos_cloro.py

# Paso 2: Entrenar modelo
python modelo_cloro_lstm.py
```

## ğŸ—ï¸ Arquitectura del Modelo

### Modelo LSTM (7 capas)

```
Capa 1: Dense(64, activation='relu')        # ProyecciÃ³n inicial
Capa 2: LSTM(64, return_sequences=True)      # Primera LSTM
Capa 3: Dropout(0.2)                         # RegularizaciÃ³n
Capa 4: LSTM(64)                             # Segunda LSTM
Capa 5: Dropout(0.2)                         # RegularizaciÃ³n
Capa 6: Dense(32, activation='relu')         # Capa intermedia
Capa 7: Dense(1)                             # Salida escalar
```

### HiperparÃ¡metros

| ParÃ¡metro | Valor | JustificaciÃ³n |
|-----------|-------|---------------|
| Hidden Units | 64 | Balance entre capacidad y complejidad |
| Dropout Rate | 0.2 | Previene overfitting |
| Learning Rate | 0.001 | Convergencia estable |
| Batch Size | 16 | Apropiado para ~800 muestras |
| Ã‰pocas MÃ¡ximas | 200 | Suficiente para convergencia |
| Early Stopping Patience | 15 | Restaura mejores pesos |

## ğŸ“Š Proceso de Modelado

### 1. GeneraciÃ³n de Datos
- Sistema: EDO de concentraciÃ³n de cloro en tanque
- ParÃ¡metros fÃ­sicos: k=0.15, C_entrada=2.0 mg/L
- Muestras: 1000 puntos temporales
- Ruido: Gaussiano (Ïƒ=0.02) para simular mediciones reales

### 2. CÃ¡lculo de ParÃ¡metros de Embebimiento

**Ï„ (Retardo Temporal):**
- MÃ©todo: AutocorrelaciÃ³n
- Criterio: Primer cruce por cero
- Rango esperado: [1, 10]

**d (DimensiÃ³n de Embebimiento):**
- MÃ©todo: False Nearest Neighbors (FNN)
- Criterio: FNN < 5%
- Rango esperado: [4, 8]

### 3. DivisiÃ³n de Datos (SIN SHUFFLING)

```
Total: 100%
â”œâ”€â”€ Train:      68%  (para aprendizaje)
â”œâ”€â”€ Validation: 12%  (para early stopping)
â””â”€â”€ Test:       20%  (para evaluaciÃ³n final)
```

### 4. Entrenamiento

- **Optimizer:** Adam (lr=0.001)
- **Loss:** MSE (Mean Squared Error)
- **Metric:** MAE (Mean Absolute Error)
- **Callback:** EarlyStopping (monitor='val_loss', patience=15)

### 5. EvaluaciÃ³n

**MÃ©tricas Cuantitativas:**
- **RMSE** (Root Mean Squared Error)
- **MAE** (Mean Absolute Error)
- Expresadas como porcentaje del rango

**Criterios de Ã‰xito:**
- âœ“ RMSE < 5% del rango
- âœ“ MAE < 3% del rango
- âœ“ PredicciÃ³n sigue observado visualmente

## ğŸ“ˆ Resultados Esperados

### Visualizaciones Generadas

1. **Serie Temporal Completa** (`datos_cloro_visualizacion.png`)
   - Serie temporal original
   - DistribuciÃ³n de valores

2. **Resultados del Modelo** (`lstm_cloro_resultados.png`)
   - PredicciÃ³n vs Observado (Test Set)
   - Curvas de aprendizaje (Train/Val Loss)
   - Error de predicciÃ³n
   - DistribuciÃ³n del error

### MÃ©tricas TÃ­picas

Para el sistema de cloro (comportamiento suave y predecible):

```
RMSE: ~2-4% del rango
MAE:  ~1-3% del rango
Estado: âœ“ APROBADO
```

## âœ… Reglas Cumplidas

### Reglas Generales
- **G1:** SeparaciÃ³n caja negra (ecuaciones solo en generaciÃ³n, NO en entrenamiento)
- **G2:** NormalizaciÃ³n MinMaxScaler a [0, 1]
- **G3:** Sin shuffling en datos temporales
- **G4:** DocumentaciÃ³n completa de hiperparÃ¡metros

### Reglas de Embebimiento (Takens)
- **T1:** CÃ¡lculo de Ï„ por autocorrelaciÃ³n
- **T2:** CÃ¡lculo de d por FNN
- **T3:** ConstrucciÃ³n de matriz embebida

### Reglas LSTM
- **L1:** Arquitectura de 7 capas especificada
- **L2:** CompilaciÃ³n con Adam, MSE, MAE
- **L3:** Split 68/12/20 sin shuffling
- **L4:** Entrenamiento con early stopping
- **L5:** EvaluaciÃ³n en test set

### Reglas de ValidaciÃ³n
- **V1:** MÃ©tricas numÃ©ricas (RMSE, MAE)
- **V2:** ValidaciÃ³n visual con grÃ¡ficas

## ğŸ” InterpretaciÃ³n de Resultados

### Â¿QuÃ© significan las mÃ©tricas?

- **RMSE < 5%:** El modelo predice con alta precisiÃ³n
- **PredicciÃ³n sigue observado:** La dinÃ¡mica fue capturada correctamente
- **Convergencia sin overfitting:** Early stopping funcionÃ³ correctamente

### Â¿Por quÃ© funciona sin ecuaciones?

El **Teorema de Takens** garantiza que:
1. La serie temporal contiene toda la informaciÃ³n del sistema
2. El embebimiento reconstruye el espacio de fases
3. La LSTM aprende las transiciones de estado

## ğŸ§ª ValidaciÃ³n CientÃ­fica

El modelo es vÃ¡lido si:
1. âœ“ RMSE < 5% del rango
2. âœ“ LÃ­nea de predicciÃ³n sigue observado visualmente
3. âœ“ No hay divergencia en los primeros 5 pasos
4. âœ“ Convergencia suave (sin overfitting agudo)

## ğŸ“š Referencias

- Takens, F. (1981). "Detecting strange attractors in turbulence"
- Hochreiter & Schmidhuber (1997). "Long Short-Term Memory"
- Kennel, Brown & Abarbanel (1992). "Determining embedding dimension"

## ğŸ‘¤ Autor

Proyecto desarrollado siguiendo la guÃ­a tÃ©cnica para implementaciÃ³n de modelos IA en sistemas dinÃ¡micos.

## ğŸ“„ Licencia

Proyecto acadÃ©mico - Universidad

---

## ğŸ†˜ SoluciÃ³n de Problemas

### Error: "datos_cloro.csv no encontrado"
**SoluciÃ³n:** Ejecutar primero `python generar_datos_cloro.py`

### Advertencias de TensorFlow
**SoluciÃ³n:** Normal, se pueden ignorar

### Convergencia lenta
**SoluciÃ³n:** Verificar que Ï„ y d sean razonables (Ï„ < 10, d < 10)

### RMSE > 5%
**Causas posibles:**
- Datos insuficientes
- HiperparÃ¡metros no optimizados
- Early stopping muy agresivo

---

**Â¡El modelo estÃ¡ listo para usar!** ğŸ‰
