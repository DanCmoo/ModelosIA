# Modelos de IA para Sistemas DinÃ¡micos

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa modelos de inteligencia artificial para predicciÃ³n de sistemas dinÃ¡micos usando el **Teorema de Takens** para reconstrucciÃ³n del espacio de fases. Incluye dos sistemas con diferentes caracterÃ­sticas:

1. **Cloro (Sistema Predecible)** - LSTM
2. **Hindmarsh-Rose (Sistema CaÃ³tico)** - Echo State Network (ESN)

El proyecto sigue estrictamente las reglas y especificaciones de la guÃ­a tÃ©cnica, demostrando que la IA puede aprender dinÃ¡micas **sin conocer las ecuaciones subyacentes** (enfoque de caja negra).

## ğŸ¯ Objetivos

- Predecir comportamientos de sistemas dinÃ¡micos usando solo datos observados
- Comparar arquitecturas apropiadas para sistemas predecibles vs caÃ³ticos
- Demostrar la validez del Teorema de Takens en modelado de caja negra
- Cumplir con todas las reglas de implementaciÃ³n (G1-G4, T1-T3, L1-L5/E1-E7, V1-V5)

## ğŸ”¬ Fundamento TeÃ³rico

### Teorema de Takens
El proyecto se basa en el teorema de reconstrucciÃ³n del espacio de fases, que establece que una serie temporal escalar `s(t)` puede ser embebida en un espacio de dimensiÃ³n superior que preserva las propiedades dinÃ¡micas del sistema original:

**X**áµ¢ = [s(i), s(i+Ï„), s(i+2Ï„), ..., s(i+(d-1)Ï„)]

Donde:
- **Ï„** = retardo temporal (calculado por autocorrelaciÃ³n)
- **d** = dimensiÃ³n de embebimiento (calculado por False Nearest Neighbors)

## ğŸ“ Estructura del Proyecto

```
ModelosIA/
â”œâ”€â”€ cloro/                           # Modelo de ConcentraciÃ³n de Cloro (LSTM)
â”‚   â”œâ”€â”€ generar_datos_cloro.py      # Genera serie temporal y guarda CSV
â”‚   â”œâ”€â”€ modelo_cloro_lstm.py         # Modelo LSTM principal
â”‚   â”œâ”€â”€ ejecutar_pipeline.py         # Script para ejecutar todo el pipeline
â”‚   â”œâ”€â”€ comparar_modelos.py          # ComparaciÃ³n modelo fÃ­sico vs LSTM
â”‚   â”œâ”€â”€ README.md                    # DocumentaciÃ³n del modelo de cloro
â”‚   â””â”€â”€ [archivos generados...]
â”œâ”€â”€ hindmarsh_rose/                  # Modelo de Hindmarsh-Rose (ESN)
â”‚   â”œâ”€â”€ generar_datos_hindmarsh_rose.py  # Genera serie caÃ³tica y guarda CSV
â”‚   â”œâ”€â”€ modelo_hindmarsh_rose_esn.py     # Modelo ESN principal
â”‚   â”œâ”€â”€ ejecutar_pipeline.py         # Script para ejecutar todo el pipeline
â”‚   â”œâ”€â”€ README.md                    # DocumentaciÃ³n del modelo H-R
â”‚   â””â”€â”€ [archivos generados...]
â”œâ”€â”€ documentos/
â”‚   â””â”€â”€ guia_tecnica_reglas.md       # GuÃ­a de implementaciÃ³n completa
â”œâ”€â”€ pyproject.toml                   # ConfiguraciÃ³n UV/Python
â”œâ”€â”€ requirements.txt                 # Dependencias (pip)
â”œâ”€â”€ INSTALACION_UV.md                # GuÃ­a de instalaciÃ³n con UV
â””â”€â”€ README.md                        # Este archivo
```

## ğŸš€ InstalaciÃ³n

### Requisitos
- Python 3.8+
- TensorFlow 2.x
- NumPy, Pandas, Scikit-learn, Matplotlib, SciPy

### OpciÃ³n 1: Instalar con UV (Recomendado - âš¡ MÃ¡s rÃ¡pido)

```bash
# Instalar UV si no lo tienes
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"

# Instalar dependencias
uv venv
uv pip install -e .
```

Ver [INSTALACION_UV.md](INSTALACION_UV.md) para mÃ¡s detalles.

### OpciÃ³n 2: Instalar con pip

```bash
pip install -r requirements.txt
```

## ğŸ’» Uso

### Modelo de Cloro (LSTM)

```bash
# Navegar a la carpeta del modelo
cd cloro

# Con UV (Recomendado)
uv run --no-project ejecutar_pipeline.py

# O paso a paso
uv run --no-project generar_datos_cloro.py
uv run --no-project modelo_cloro_lstm.py
uv run --no-project comparar_modelos.py  # ComparaciÃ³n con modelo fÃ­sico
```

### Modelo de Hindmarsh-Rose (ESN)

```bash
# Navegar a la carpeta del modelo
cd hindmarsh_rose

# Con UV (Recomendado)
uv run --no-project ejecutar_pipeline.py

# O paso a paso
uv run --no-project generar_datos_hindmarsh_rose.py
uv run --no-project modelo_hindmarsh_rose_esn.py
```

## ğŸ—ï¸ ComparaciÃ³n de Arquitecturas

### Cloro vs Hindmarsh-Rose

| Aspecto | Cloro (LSTM) | Hindmarsh-Rose (ESN) |
|---------|--------------|---------------------|
| **Comportamiento** | Suave, predecible | CaÃ³tico, impredecible |
| **Arquitectura** | LSTM (6 capas) | ESN (Reservoir Computing) |
| **ParÃ¡metros entrenables** | ~52,000 | ~300 (solo W_out) |
| **Entrenamiento** | Backpropagation iterativo | Ridge Regression (soluciÃ³n cerrada) |
| **Tiempo de entrenamiento** | Minutos (~38 Ã©pocas) | Segundos |
| **MÃ©trica clave** | RMSE < 5% | Horizonte Lyapunov > 5 pasos |
| **PredicciÃ³n largo plazo** | Precisa (many-step) | Limitada (efecto mariposa) |
| **Reservoir size** | N/A | 300 neuronas |
| **Spectral radius** | N/A | 0.9 (edge of chaos) |

### Modelo LSTM para Cloro

```
Arquitectura: 6 capas
â”œâ”€ Reshape(input_dim, 1)
â”œâ”€ LSTM(64, return_sequences=True)
â”œâ”€ Dropout(0.2)
â”œâ”€ LSTM(64)
â”œâ”€ Dropout(0.2)
â”œâ”€ Dense(32, activation='relu')
â””â”€ Dense(1)

ParÃ¡metros: ~52,000
Entrenamiento: Adam optimizer + Early Stopping
```

### Modelo ESN para Hindmarsh-Rose

```
Arquitectura: Reservoir Computing
â”œâ”€ W_in (input â†’ reservoir): Fijo, aleatorio
â”œâ”€ W_res (reservoir): Fijo, Ï(W)=0.9
â””â”€ W_out (reservoir â†’ output): Entrenado (Ridge)

ParÃ¡metros entrenables: 300
Entrenamiento: SoluciÃ³n cerrada (Ridge Regression)
```

## ğŸ“Š Proceso de Modelado

### 1. GeneraciÃ³n de Datos
- Sistema: EDO de concentraciÃ³n de cloro en tanque
- ParÃ¡metros fÃ­sicos: k=0.15, C_entrada=2.0 mg/L
- Muestras: 1000 puntos temporales
- Ruido: Gaussiano (Ïƒ=0.02) para simular mediciones reales

### 2. CÃ¡lculo de ParÃ¡metros de Embebimiento

**Ï„ (Retardo Temporal):**
- MÃ©todo: AutocorrelaciÃ³n
- Criterio: Primer cruce por cero
- Rango esperado: [1, 10]

**d (DimensiÃ³n de Embebimiento):**
- MÃ©todo: False Nearest Neighbors (FNN)
- Criterio: FNN < 5%
- Rango esperado: [4, 8]

### 3. DivisiÃ³n de Datos (SIN SHUFFLING)

```
Total: 100%
â”œâ”€â”€ Train:      68%  (para aprendizaje)
â”œâ”€â”€ Validation: 12%  (para early stopping)
â””â”€â”€ Test:       20%  (para evaluaciÃ³n final)
```

### 4. Entrenamiento

- **Optimizer:** Adam (lr=0.001)
- **Loss:** MSE (Mean Squared Error)
- **Metric:** MAE (Mean Absolute Error)
- **Callback:** EarlyStopping (monitor='val_loss', patience=15)

### 5. EvaluaciÃ³n

**MÃ©tricas Cuantitativas:**
- **RMSE** (Root Mean Squared Error)
- **MAE** (Mean Absolute Error)
- Expresadas como porcentaje del rango

**Criterios de Ã‰xito:**
- âœ“ RMSE < 5% del rango
- âœ“ MAE < 3% del rango
- âœ“ PredicciÃ³n sigue observado visualmente

## ğŸ“ˆ Resultados Esperados

### Visualizaciones Generadas

1. **Serie Temporal Completa** (`datos_cloro_visualizacion.png`)
   - Serie temporal original
   - DistribuciÃ³n de valores

2. **Resultados del Modelo** (`lstm_cloro_resultados.png`)
   - PredicciÃ³n vs Observado (Test Set)
   - Curvas de aprendizaje (Train/Val Loss)
   - Error de predicciÃ³n
   - DistribuciÃ³n del error

### MÃ©tricas TÃ­picas

Para el sistema de cloro (comportamiento suave y predecible):

```
RMSE: ~2-4% del rango
MAE:  ~1-3% del rango
Estado: âœ“ APROBADO
```

## âœ… Reglas Cumplidas

### Reglas Generales
- **G1:** SeparaciÃ³n caja negra (ecuaciones solo en generaciÃ³n, NO en entrenamiento)
- **G2:** NormalizaciÃ³n MinMaxScaler a [0, 1]
- **G3:** Sin shuffling en datos temporales
- **G4:** DocumentaciÃ³n completa de hiperparÃ¡metros

### Reglas de Embebimiento (Takens)
- **T1:** CÃ¡lculo de Ï„ por autocorrelaciÃ³n
- **T2:** CÃ¡lculo de d por FNN
- **T3:** ConstrucciÃ³n de matriz embebida

### Reglas LSTM
- **L1:** Arquitectura de 7 capas especificada
- **L2:** CompilaciÃ³n con Adam, MSE, MAE
- **L3:** Split 68/12/20 sin shuffling
- **L4:** Entrenamiento con early stopping
- **L5:** EvaluaciÃ³n en test set

### Reglas de ValidaciÃ³n
- **V1:** MÃ©tricas numÃ©ricas (RMSE, MAE)
- **V2:** ValidaciÃ³n visual con grÃ¡ficas

## ğŸ” InterpretaciÃ³n de Resultados

### Â¿QuÃ© significan las mÃ©tricas?

- **RMSE < 5%:** El modelo predice con alta precisiÃ³n
- **PredicciÃ³n sigue observado:** La dinÃ¡mica fue capturada correctamente
- **Convergencia sin overfitting:** Early stopping funcionÃ³ correctamente

### Â¿Por quÃ© funciona sin ecuaciones?

El **Teorema de Takens** garantiza que:
1. La serie temporal contiene toda la informaciÃ³n del sistema
2. El embebimiento reconstruye el espacio de fases
3. La LSTM aprende las transiciones de estado

## ğŸ§ª ValidaciÃ³n CientÃ­fica

El modelo es vÃ¡lido si:
1. âœ“ RMSE < 5% del rango
2. âœ“ LÃ­nea de predicciÃ³n sigue observado visualmente
3. âœ“ No hay divergencia en los primeros 5 pasos
4. âœ“ Convergencia suave (sin overfitting agudo)

## ğŸ“š Referencias

- Takens, F. (1981). "Detecting strange attractors in turbulence"
- Hochreiter & Schmidhuber (1997). "Long Short-Term Memory"
- Kennel, Brown & Abarbanel (1992). "Determining embedding dimension"

## ğŸ‘¤ Autor

Proyecto desarrollado siguiendo la guÃ­a tÃ©cnica para implementaciÃ³n de modelos IA en sistemas dinÃ¡micos.

## ğŸ“„ Licencia

Proyecto acadÃ©mico - Universidad

---

## ğŸ†˜ SoluciÃ³n de Problemas

### Error: "datos_cloro.csv no encontrado"
**SoluciÃ³n:** Ejecutar primero `python generar_datos_cloro.py`

### Advertencias de TensorFlow
**SoluciÃ³n:** Normal, se pueden ignorar

### Convergencia lenta
**SoluciÃ³n:** Verificar que Ï„ y d sean razonables (Ï„ < 10, d < 10)

### RMSE > 5%
**Causas posibles:**
- Datos insuficientes
- HiperparÃ¡metros no optimizados
- Early stopping muy agresivo

---

**Â¡El modelo estÃ¡ listo para usar!** ğŸ‰
